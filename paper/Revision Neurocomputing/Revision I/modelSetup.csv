Model,R package,Running time (min),Feature enginering,Activation formula,Optimization algorithm,Main assumptions and limitations
LR,stats,~3 ,NO,"glm(marks~.,family = binomial, data=train.data)",maximum likelihood estimation,linear relationship between the response and the predictor variables
LDA,MASS,~3 ,NO,"lda(marks~., data=train.data)",singular value decomposition,normally distributed data and equal covariance class matrices
QDA,MASS,~3 ,NO,"qda(marks~., data=train.data)",maximum likelihood estimation,normally distributed data
KNN,class,~20 ,YES,"knn(Xlag[train,], Xlag[-train,], ClassTable[train,""marks""], k=3)",grid search,sensitive to the choice of the hyperparameter k
LASSO,glmnet,~10 ,NO,"cv.glmnet(x, y, family=""binomial"", type.measure=""auc"")",coordinate descent method,linear relationship between the response and the predictor variables
GAM,gam,~12 ,NO,"gam(marks~. + s(trend, df=3) + s(seas_acf1, df = 3) + s(linearity, df = 3) + s(entropy, df = 3) + s(x_acf1, df = 3) + s(seasonal_strength, df = 3) + s(seas_acf1, df = 3), family = binomial, data = train.data)",penalized regression maximum likelihood estimation,additive relationship between the predictor variables and the response variable
RF,randomForest,~2200 ,NO,"randomForest(marks~., data=train.data1, mtry=10, ntree=3000, importance = TRUE)",bootstrap aggregation,"parsimony, overfitting, interpretability and long computation time"
Boosting,gbm,~3000 ,YES,"gbm(marks~., data = train.data01, distribution = ""bernoulli"", n.trees = 4500, shrinkage = 0.01, interaction.depth = 10)",gradient boosting,"parsimony, overfitting, interpretability and long computation time"
SVM,e1071,~6000 ,YES,"tune(e1071::svm, as.factor(marks)~., data = train.data, scale = FALSE, kernel = ""radial"", ranges = list(cost=c(0.001 , 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5, 1, 2, 3, 4)))",Lagrange multipliers for constrained optimization problem,"interpretability, sensitivity to kernel choice, assumes that the classes can be separated by a linear boundary or a hyperplane in high-dimensional space, long computation time"
FNN,nnet,~10 ,YES,"nnet(marks~., data=train.data, size=10, maxit=500, decay=0.001, rang = 0.1)",resilient backpropagation algorithm,"interpretability, limited flexibility, choice of hyperparameters and overfitting"
DTW,dtw,~5 ,YES,"knn(train = trainData[, -5], test = testData[, -5], cl = trainLabels, k = k, prob = TRUE, use.all = TRUE, distance = dtw.dist)",dynamic programming algorithm,"interpretability, computational complexity, sensitivity to noise and choice of warping function"
DL Torch,torch,~180 ,YES,"nn_module(  ""Net"",  initialize = function() {  self$fc1 <- nn_linear(length(features1), 40),  self$fc2 <- nn_linear(40, 30),  self$fc3 <- nn_linear(30, 30), self$fc4 <- nn_linear(30, 15),  self$fc5 <- nn_linear(15, 15),  self$fc6 <- nn_linear(15, 10), self$fc7 <- nn_linear(10, 10),  self$fc8 <- nn_linear(10, 5),  self$fc9 <- nn_linear(5, 5),  self$fc10 <- nn_linear(5, 1) },
  forward = function(x) {  x %>% 
      self$fc1() %>%  nnf_relu() %>%  self$fc2() %>% nnf_relu() %>% self$fc3() %>%  nnf_relu() %>% self$fc4() %>% nnf_relu() %>% self$fc5() %>% nnf_relu() %>% self$fc6() %>%      nnf_relu() %>%  self$fc7() %>%  nnf_relu() %>%  self$fc8() %>%  nnf_relu() %>% self$fc9() %>%  nnf_relu() %>% self$fc10() })",adaptive moment estimation,"interpretability, computational complexity, choice of hyperparameters, overfitting and long computation time"
XG Boost,xgboost,~300 ,YES,"xgboost(data = xgboost_train, max.depth=23, nrounds=500, objective = ""binary:logistic"")",gradient boosting with decision trees,"interpretability, computational complexity, choice of hyperparameters, overfitting and long computation time"
TensorFlow,"tensorflow, keras",~30 ,YES,"keras_model_sequential() %>% layer_dense(units = 80, activation = ""relu"", input_shape = c(42)) %>%  layer_dropout(rate = 0.6) %>%  layer_dense(units = 40, activation = ""relu"") %>%  layer_dropout(rate = 0.3) %>%  layer_dense(units = 5, activation = ""relu"") %>%   layer_dense(units = 1, activation = ""sigmoid"")",root mean square propagation,"interpretability, computational complexity, choice of hyperparameters and overfitting"
RNN,keras,~30 ,YES,"keras_model_sequential() %>%  layer_lstm(units = 64, input_shape = c(42, 1)) %>%  layer_dropout(rate = 0.2) %>%  layer_dense(units = 1, activation = ""sigmoid"")",adaptive moment estimation,"temporal dependence between the input and output variables, interpretability, computational complexity, choice of hyperparameters and overfitting"
CNN,keras,~35 ,YES,"keras_model_sequential() %>%  layer_conv_1d(filters = 32, kernel_size = 3, activation = ""relu"", input_shape = c(42, 1)) %>%  layer_max_pooling_1d(pool_size = 2) %>%
layer_dropout(rate = 0.2) %>%  layer_flatten() %>%  layer_dense(units = 1, activation = ""sigmoid"")",adaptive moment estimation,"spatial dependence between the input and output variables, interpretability, computational complexity, choice of hyperparameters and overfitting"