Applied to decision
trees, the use of the gradient boosting technique results in models that strictly outperform
random forests most of the time, while having similar properties. It may be one of the
best, if not  best, algorithm for dealing with nonperceptual data today. Alongside deepthe
learning, it’s one of the most commonly used techniques in Kaggle competitions.

Ovo reci u kontekstu ako RF and DP budu jedan do drugog onda to i citiraj 19.

Since 2012, deep convolutional neural networks ( ) have become the go-toconvnets
algorithm for all computer vision tasks; more generally, they work on all perceptual
tasks. At major computer vision conferences in 2015 and 2016, it was nearly impossible
to find presentations that didn’t involve convnets in some form. At the same time, deep
learning has also found applications in many other types of problems, such as natural
language processing. It has completely replaced SVMs and decision trees in a wide range
of applications. For instance, for several years, the European Organization for Nuclear
Research, CERN, used decision tree–based methods for analysis of particle data from the
ATLAS detector at the Large Hadron Collider (LHC); but CERN eventually switched to
Keras-based deep neural networks due to their higher performance and ease of training
on large datasets.

Argument da se konvolucijske koriste za klasifikaciju slike! 20 strana.

In 2016, Kaggle was dominated by two approaches: gradient boosting machines and
deep learning. 

XGBoost
library

Theano and then TensorFlow—two
symbolic tensor-manipulation frameworks that support auto-differentiation, greatly
simplifying the implementation of new models—and by the rise of user-friendly libraries
such as Keras, which makes deep learning as easy as manipulating LEGO bricks. After
its release early 2015, Keras quickly became the go-to deep-learning solution for large
numbers of new startups, grad students, and researchers pivoting into the field.